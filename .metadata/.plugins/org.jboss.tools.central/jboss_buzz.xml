<?xml version="1.0" encoding="UTF-8"?>
<feed xmlns="http://www.w3.org/2005/Atom" xmlns:dc="http://purl.org/dc/elements/1.1/"><title>JBoss Tools Aggregated Feed</title><link rel="alternate" href="http://tools.jboss.org" /><subtitle>JBoss Tools Aggregated Feed</subtitle><dc:creator>JBoss Tools</dc:creator><entry><title>Quarkus Newsletter #36 - September</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-newsletter-36/&#xA;            " /><author><name>James Cobb (https://twitter.com/insectengine)</name></author><id>https://quarkus.io/blog/quarkus-newsletter-36/</id><updated>2023-09-15T00:00:00Z</updated><published>2023-09-15T00:00:00Z</published><summary type="html">Explore how we can use the Testcontainers Desktop app while building a Quarkus application by reading "Joyful Quarkus Application Development using Testcontainers Desktop" by Siva Katamreddy. Extensions can significantly increase the application’s performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the...</summary><dc:creator>James Cobb (https://twitter.com/insectengine)</dc:creator><dc:date>2023-09-15T00:00:00Z</dc:date></entry><entry><title>How to configure RHEL as a workstation during installation</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/14/how-configure-rhel-workstation-during-installation" /><author><name>Nikhil Mungale</name></author><id>a410ee41-5f4f-455e-91fe-5183739593cf</id><updated>2023-09-14T07:00:00Z</updated><published>2023-09-14T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt; (RHEL) is a powerful and widely-used &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; distribution known for its stability, security features, and enterprise-grade support. When installing RHEL, you have the opportunity to configure it as a workstation to optimize performance and usability for your specific needs. In this article, we will guide you through the steps to configure RHEL as a workstation during the installation process.&lt;/p&gt; &lt;h2&gt;What is RHEL workstation?&lt;/h2&gt; &lt;p&gt;The organization needs high-end workstations that can support developers and power users, such as artists, physicians, scientists, and engineers, so they can concentrate on what they do best. Designed for advanced Linux users and day-to-day usage with powerful hardware, &lt;a href="https://www.redhat.com/en/store/red-hat-enterprise-linux-workstation"&gt;Red Hat Enterprise Linux for Workstations&lt;/a&gt; (RHEL Workstation) is optimized for high-performance graphics, animation, and scientific applications. It includes all the capabilities and applications that workstation users need, plus development tools for provisioning and administration.&lt;/p&gt; &lt;h2&gt;How to install and configure a RHEL workstation&lt;/h2&gt; &lt;p&gt;The following sections will demonstrate how to configure RHEL as a workstation during the installation process.&lt;/p&gt; &lt;h3&gt;Prerequisites&lt;/h3&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Red Hat Enterprise Linux installation media or ISO file.&lt;/li&gt; &lt;li aria-level="1"&gt;Bootable USB/DVD drive containing.ISO file of RHEL 9.2.&lt;/li&gt; &lt;li aria-level="1"&gt;Activated no-cost &lt;a href="developer.redhat.com"&gt;Red Hat Developer subscription&lt;/a&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;A system that meets the following requirements: &lt;ul&gt;&lt;li aria-level="2"&gt;A 64-bit x86 or ARM machine&lt;/li&gt; &lt;li aria-level="2"&gt;4 GB of RAM&lt;/li&gt; &lt;li aria-level="2"&gt;At least 20 GB of available disk space(50 GB for best results)&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Prepare Windows system for RHEL installation&lt;/h3&gt; &lt;p&gt;We are making a single machine with two operating systems, RHEL workstation and Windows 11. We need to isolate the operating systems from each other to install RHEL on the same disk where Windows OS is already installed.&lt;/p&gt; &lt;p&gt;Follow these steps required to make a single disk into multiple partitions:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Right-click on the Windows icon and select the disk management option, as shown in Figure 1.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture1_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture1_0.png?itok=N9SnoOfv" width="596" height="497" alt="Disk management" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1:  Selecting Disk Management.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ol start="2"&gt;&lt;li aria-level="1"&gt;You will see the window with all drives shown graphically.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the drive which has enough space to make a partition.&lt;/li&gt; &lt;li aria-level="1"&gt;Right-click on drive and select the &lt;strong&gt;Shrink Volume&lt;/strong&gt; option.&lt;/li&gt; &lt;li aria-level="1"&gt;In the wizard, define the partition and size by entering &lt;strong&gt;Enter the amount of shrink in MB&lt;/strong&gt; and click on the &lt;strong&gt;Shrink&lt;/strong&gt; button. (Note: Define space at least 50GB for best experience.)&lt;/li&gt; &lt;li aria-level="1"&gt;The freed space should now be shown with &lt;strong&gt;Unallocated Status&lt;/strong&gt; in the bottom pane. Right-click on it and select &lt;strong&gt;New Simple Volume&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the next wizard, select &lt;strong&gt;Do not assign a drive letter or drive path &lt;/strong&gt;and click on &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;In the &lt;strong&gt;Format Partition&lt;/strong&gt; window, keep everything as default and change the&lt;strong&gt; Volume label&lt;/strong&gt; new volume to &lt;strong&gt;RHEL.&lt;/strong&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Finish&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;After completing the setup, you will see the disk partition for RHEL in Figure 2.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture2_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture2_0.png?itok=-Hbehbyh" width="600" height="257" alt="disk" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Disk partition for RHEL OS&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Start installing RHEL&lt;/h3&gt; &lt;p&gt;To begin installation on RHEL, make sure a bootable USB/DVD is attached to the device.&lt;/p&gt; &lt;p&gt;Press the shift&lt;strong&gt; &lt;/strong&gt;key and restart&lt;strong&gt; &lt;/strong&gt;the system, as shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture3_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture3_0.png?itok=VGosab6M" width="493" height="436" alt="restart" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Restart the system&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;During system boot up, the screen in Figure 4 appears. Click on the&lt;strong&gt; Use a device &lt;/strong&gt;option.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture4_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture4_0.png?itok=02Yixx_C" width="600" height="370" alt="boot option" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 4:  Choosing a boot option.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;Select the&lt;strong&gt; Linpus lite (USB&lt;/strong&gt;) option using navigation keys from the keyboard and press enter, as shown in Figure 5.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture5_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture5_0.png?itok=pGca53f4" width="600" height="405" alt="USB" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 5: Select the USB boot device&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The screen in Figure 6 appears to choose the RHEL installation option, and boot the system using bootable installation media containing the RHEL 9.iso file. For this article, we will use &lt;strong&gt;Red Hat Enterprise Linux 9.2.&lt;/strong&gt; Then press enter.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture6_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture6_0.png?itok=4v1Vvns4" width="600" height="453" alt="rhel select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 6: Select RHEL to install.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;strong&gt;NOTE&lt;/strong&gt;: During booting, you can skip the media checking step by hitting the &lt;strong&gt;Esc&lt;/strong&gt; key.&lt;/p&gt; &lt;p&gt;As shown in Figure 7, select the language of RHEL to install and click &lt;strong&gt;Continue&lt;/strong&gt;.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture7_0.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture7_0.png?itok=e1blA1Zw" width="600" height="449" alt="language select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 7: Selecting the OS language.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure RHEL&lt;/h3&gt; &lt;p&gt;During installation, we must manually configure RHEL.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Connect to Red Hat&lt;/strong&gt;, as shown in Figure 8.&lt;/li&gt; &lt;li aria-level="1"&gt;Fill in your username&lt;strong&gt; &lt;/strong&gt;and password&lt;strong&gt; &lt;/strong&gt;for the activated no-cost Red Hat Developer subscription.&lt;/li&gt; &lt;li aria-level="1"&gt;Leave the organization blank (optional).&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Register &lt;/strong&gt;button.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture8.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture8.png?itok=Df1CXQvA" width="600" height="426" alt="config select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 8: The RHEL configuration screen. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Choose the disk and partition&lt;/h3&gt; &lt;p&gt;The &lt;strong&gt;Installation Destination&lt;/strong&gt; has a caution sign. To install RHEL, we must select the drive. There are two ways to install RHEL on disk with a dedicated disk and partitioned disk space.&lt;/p&gt; &lt;h4&gt;Dedicated disk&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on &lt;strong&gt;Installation Destination&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;It shows all available disks. Select the disk on which you want to install RHEL.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;h4&gt;Partitioned disk&lt;/h4&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Get into the &lt;strong&gt;Installation Destination&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on disk and select the claim drive.&lt;/li&gt; &lt;li aria-level="1"&gt;In the window in Figure 9, you will see all the partitions of the hard disk.&lt;/li&gt; &lt;li aria-level="1"&gt;Select &lt;strong&gt;RHEL&lt;/strong&gt; labeled drive.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on &lt;strong&gt;Delete &lt;/strong&gt;to the left.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the &lt;strong&gt;Reclaim space&lt;/strong&gt; button.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture9.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture9.png?itok=vdhsbR1r" width="600" height="450" alt="disk select" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 9: Choose the disk for OS installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Configure the network and host name&lt;/h3&gt; &lt;p&gt;Next, configure the system network.&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Network &amp; HostName&lt;/strong&gt; under &lt;strong&gt;SYSTEM&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;Turn on the adaptor by clicking the toggle switch.&lt;/li&gt; &lt;li aria-level="1"&gt;If you have built-in WIFI in the system, it shows the Ethernets below as a secondary option to connect to the internet.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;&lt;strong&gt;Note&lt;/strong&gt;: If you do not enable the network here, you will not be able to install RHEL, because it downloads all dependencies over the internet during the installation process.&lt;/p&gt; &lt;h3&gt;Set the root password&lt;/h3&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Scroll down to the &lt;strong&gt;Root Password&lt;/strong&gt; under the user setting and click on it.&lt;/li&gt; &lt;li aria-level="1"&gt;Next, add the password two times. Make sure it is a strong password. Based on the complexity of your password, it will show the strength of the password under it.&lt;/li&gt; &lt;/ol&gt;&lt;h3&gt;Select the software&lt;/h3&gt; &lt;p&gt;RHEL gives the flexibility to install additional packages and tools during the installation process so users can get most of the things in a single go. Refer to Figure 10 for the following steps:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Software Selection&lt;/strong&gt; button.&lt;/li&gt; &lt;li aria-level="1"&gt;Select the &lt;strong&gt;Workstation &lt;/strong&gt;under the &lt;strong&gt;Base Environment&lt;/strong&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;From &lt;strong&gt;Additional software for Selected Environment,&lt;/strong&gt; select the tools of your preference, such as container management, security tools, etc.&lt;/li&gt; &lt;li aria-level="1"&gt;Click &lt;strong&gt;Done&lt;/strong&gt;.&lt;/li&gt; &lt;/ol&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture10.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture10.png?itok=WCJRDTIL" width="600" height="458" alt="select workstation" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 10: Selecting workstation as the RHEL version. &lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Install the RHEL Workstation&lt;/h3&gt; &lt;p&gt;After completing all configurations, we are ready to begin the installation of RHEL Workstation.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Click the &lt;strong&gt;Begin installation&lt;/strong&gt; button shown in Figure 11 to start the installation.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture11.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture11.png?itok=2h5gV_Nt" width="600" height="450" alt="all config done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 11:  Begin the installation.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The installation may take some time based on your internet speed and compute power.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;After installation, we need to reboot the system, by clicking on the &lt;strong&gt;Reboot System&lt;/strong&gt; button in Figure 12.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture12.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture12.png?itok=Vv5vSrrd" width="600" height="443" alt="installation done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 12: Rebooting the system.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Set up RHEL&lt;/h3&gt; &lt;p&gt;After the successful installation of RHEL, you will see the screen in Figure 13.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Simply click &lt;strong&gt;Start Setup&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture13.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture13.png?itok=HwquSu0y" width="600" height="456" alt="rhel setup" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 13:  Start the RHEL setup.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;RHEL requires more inputs before using it.&lt;/li&gt; &lt;li aria-level="1"&gt;Accept the policy of RHEL and click on next.&lt;/li&gt; &lt;li aria-level="1"&gt;Add the accounts of Google and Microsoft or skip it and click on Next.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a new user ID (Figure 14). Fill in the &lt;strong&gt;Full Name&lt;/strong&gt; and &lt;strong&gt;Username &lt;/strong&gt;boxes and click &lt;strong&gt;Next&lt;/strong&gt;.&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture14.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture14.png?itok=tQPQ7HZ9" width="600" height="445" alt="Create a user" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 14: Create a user.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;On the next screen, set the password for the login system. Make sure the password is strong.&lt;/li&gt; &lt;li aria-level="1"&gt;Click on the &lt;strong&gt;Start Using Red Hat Enterprise Linux&lt;/strong&gt; (Figure 15).&lt;/li&gt; &lt;/ul&gt;&lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture15.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture15.png?itok=Cf5G3uA8" width="600" height="450" alt="click on start" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 15: Click the Start Using Red Hat Enterprise Linux button.&lt;/figcaption&gt;&lt;/figure&gt;&lt;ul&gt;&lt;/ul&gt;&lt;p&gt;RHEL Workstation is now ready for personal and high-end use (Figure 16).&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/picture16.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/picture16.png?itok=GB0cpuQS" width="600" height="449" alt="workstation done" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 16: The desktop view of RHEL 9.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Find more resources&lt;/h2&gt; &lt;p&gt;If you want to have a more hands-on experience of RHEL, you can follow the &lt;a href="https://www.redhat.com/en/interactive-labs/enterprise-linux"&gt;Red Hat curated lab&lt;/a&gt;. Learn more with Red Hat's hands-on labs for all skill levels. Try these labs to see your favorite products in action.&lt;/p&gt; &lt;p&gt;You can also &lt;a href="https://developers.redhat.com/products/rhel/download"&gt;get customized RHEL images&lt;/a&gt; for AWS, Google Cloud Platform, Microsoft Azure, and VMware and deploy them to the platform of your choice.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/14/how-configure-rhel-workstation-during-installation" title="How to configure RHEL as a workstation during installation"&gt;How to configure RHEL as a workstation during installation&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Nikhil Mungale</dc:creator><dc:date>2023-09-14T07:00:00Z</dc:date></entry><entry><title>Quarkus security releases for CVE-2023-4853</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/cve-2023-4853/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/cve-2023-4853/</id><updated>2023-09-14T00:00:00Z</updated><published>2023-09-14T00:00:00Z</published><summary type="html">We have just released updates to Quarkus 2.16.11.Final, 3.2.6.Final, and 3.3.3 and Red Hat build of Quarkus 2.13.18.SP2 that fix the issue reported in CVE-2023-4853. This issue affects anyone using HTTP security path-based rules to protect HTTP endpoints. Recommendations If you are using any older versions of Quarkus (ranging from...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-14T00:00:00Z</dc:date></entry><entry><title>Introducing Ansible Molecule with Ansible Automation Platform</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/13/introducing-ansible-molecule-ansible-automation-platform" /><author><name>Anshul Behl</name></author><id>dd391abe-15a5-43e1-9cfd-d6b0b32bf624</id><updated>2023-09-13T07:00:00Z</updated><published>2023-09-13T07:00:00Z</published><summary type="html">&lt;p&gt;Ansible Molecule is a tool designed to aid in developing and testing Ansible playbooks, roles, and collections. It provides support for functional testing of Ansible content across multiple instances, operating systems and distributions, virtualization providers, test frameworks, and testing scenarios. Molecule helps Ansible content creators (&lt;a href="https://developers.redhat.com/topics/automation"&gt;automation&lt;/a&gt; specialists) consistently deliver automation content that is scalable, repeatable, and compatible with the latest Ansible versions.&lt;/p&gt; &lt;p&gt;Ansible Molecule 6 is now available as a &lt;a href="https://access.redhat.com/support/offerings/devpreview"&gt;developer preview&lt;/a&gt; with &lt;a href="https://developers.redhat.com/products/ansible/overview"&gt;Red Hat Ansible Automation Platform&lt;/a&gt;. This version will refocus and redefine the project as a tool for testing Ansible content with Ansible Automation Platform.&lt;/p&gt; &lt;p&gt;The developer preview enables us to collect feedback from our users as we work towards making it an integral and supported part of the Ansible Automation Platform developer experience. This release is part of our broader strategy to reduce the learning curve required for IT professionals and Ansible specialists with little to no coding skills to build, test, and deploy their automation content.&lt;/p&gt; &lt;p class="Indent1"&gt;&lt;strong&gt;Note to current Molecule users:&lt;/strong&gt; If you are already familiar with the Molecule project and are using it to test your automation content, there might be breaking changes that will require updates to your test scenarios. Please see the conclusion section of this blog to find out how to provide feedback to us.&lt;/p&gt; &lt;h2&gt;An automation testing framework built for the enterprise&lt;/h2&gt; &lt;p&gt;The latest Ansible Molecule developer preview is designed with Ansible Automation Platform and organizational level testing in mind. While it retains its core goal of providing a reliable way to make sure your automation is up to scratch, it's got some new tricks up its sleeve. Now, you can also test roles and playbooks within &lt;a href="https://docs.ansible.com/ansible/latest/dev_guide/developing_collections.html"&gt;Ansible Content Collections&lt;/a&gt;, making it even easier to develop and validate your automation content.&lt;/p&gt; &lt;p&gt;This update comes thanks to valuable feedback from our community of Molecule users. We've listened and made Molecule more user-friendly, especially for those who create and specialize in Ansible automation.&lt;/p&gt; &lt;p&gt;Here's a rundown of what's new and improved.&lt;/p&gt; &lt;h2&gt;Testing framework for content inside Ansible Content Collections&lt;/h2&gt; &lt;p&gt;Ansible Content Collections are a distribution format for Ansible content that can include playbooks, roles, modules, and plug-ins. They are used to distribute reusable Ansible content, enabling users to share, version, and distribute the building blocks of their automation.&lt;/p&gt; &lt;p&gt;Recognizing the complexity and interdependence of today's automation tasks, we have extended the capabilities of Molecule to test not just individual roles and playbooks but entire collections. This enhancement is particularly significant as we prepare for a seamless integration of Molecule into the Ansible Automation Platform.&lt;/p&gt; &lt;p&gt;The integration with Ansible Automation Platform positions Molecule as a future-ready tool. More importantly, it aligns with a user-friendly strategy for content testing within the platform. This approach aims to simplify the user experience, enabling both customers and community users to conduct comprehensive tests on their automation content in a manner that is consistent with the Ansible Automation Platform.&lt;/p&gt; &lt;h2&gt;Keeping it simple with one driver&lt;/h2&gt; &lt;p&gt;Ansible is now the default provisioner with this Molecule release. Molecule uses drivers as provisioners to create the infrastructure to run your tests on. Prior to the release of version 6, Molecule supported multiple drivers to provision testing instances using different technologies, including &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt; containers, virtual machines, and cloud providers. By default, it came with three pre-installed drivers: Docker and Podman drivers to manage &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; and a delegated driver allowing you to customize your integration using Ansible. Drivers for other providers were available through the open source community.&lt;/p&gt; &lt;p&gt;The default and the only driver present with Ansible Molecule in Ansible Automation Platform is the delegated driver (aliased as "default" driver), which allows you to use Ansible itself to create and modify how Molecule provisions its test environments.&lt;/p&gt; &lt;p&gt;Ansible can automate various environments using the Ansible collections available through the community and Ansible Automation Platform. Because the delegated ("default") driver uses Ansible, we believe that it will help the adoption of Molecule as the testing framework for the enterprise.&lt;/p&gt; &lt;p&gt;Although using other drivers was a powerful approach that allowed customizations, this approach had its disadvantages. For instance, if you want to customize a driver's provisioning/de-provisioning mechanism, you will need to change the driver's source code, which means that you will need to go through the learning curve of writing Python code even if you are an Ansible playbook writer.&lt;/p&gt; &lt;h2&gt;Making the testinfra verifier optional&lt;/h2&gt; &lt;p&gt;In the context of Ansible Molecule, a "verifier" is a component responsible for running tests against the infrastructure or instances created during the testing process. The verifier is used to validate whether the Ansible role or playbook being tested has successfully achieved the desired state on the test instances. The verifier you choose determines the testing framework and syntax you use to write your tests.&lt;/p&gt; &lt;p&gt;Ansible is a powerful verifier and has been made the default verifier with Molecule 6. Testinfra is another popular verifier with Molecule, which has been made optional with Molecule 6. The testinfra library requires Molecule users to be proficient with &lt;a href="https://developers.redhat.com/topics/python"&gt;Python&lt;/a&gt;, which limits its usability for many non-programmer IT practitioners. Making the Testinfra library optional with Molecule 6 is part of our efforts to refocus Molecule as a tool for functional testing of Ansible roles, collections, and playbooks using Ansible itself. Writing verifications can be done with native Ansible playbooks and tasks rather than using/learning a third-party tool with respect to Python.&lt;/p&gt; &lt;p&gt;What we mean by optional is that the testinfra Python library is not packaged as part of Molecule 6 for the downstream release. Rather, the testing "glue" is still available for testinfra in Molecule. For those IT practitioners who may be comfortable with Python and wish to use testinfra, it remains an installable option.&lt;/p&gt; &lt;h2&gt;Installing Molecule developer preview&lt;/h2&gt; &lt;p&gt;Molecule is packaged as part of the Ansible Automation Platform. You can &lt;a href="https://developers.redhat.com/products/ansible/download"&gt;download the bundled installer&lt;/a&gt; or &lt;a href="https://access.redhat.com/documentation/en-us/red_hat_ansible_automation_platform/2.4/html/red_hat_ansible_automation_platform_planning_guide/proc-attaching-subscriptions_planning"&gt;subscribe to the Ansible Automation Platform repos&lt;/a&gt; to get access to the supported packages.&lt;/p&gt; &lt;p&gt;You can install Molecule using the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;dnf install \ --enablerepo=ansible-automation-platform-2.4-for-rhel-8-x86_64-rpms molecule&lt;/code&gt;&lt;/pre&gt; &lt;h2&gt;Getting started with Molecule developer preview&lt;/h2&gt; &lt;p&gt;Let's take a look at how Molecule developer preview aligns more closely with Ansible content collection development and testing. All the examples here are available in the upstream Molecule &lt;a href="https://ansible.readthedocs.io/projects/molecule/getting-started/"&gt;project documentation&lt;/a&gt;.&lt;/p&gt; &lt;ol&gt;&lt;li&gt; &lt;p&gt;One of the recommended ways to create a collection is to place it under the &lt;code&gt;collections/ansible_collections&lt;/code&gt; directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy collection init foo.bar&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Navigate to the &lt;code&gt;roles&lt;/code&gt; directory in your new collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/foo.bar/roles/&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Initialize a new role for this collection:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;ansible-galaxy role init my_role&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add a task under &lt;code&gt;my_role/tasks/main.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--- - name: Task is running from within the role ansible.builtin.debug: msg: "This is a task from my_role." &lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Add Molecule to the content collection:&lt;/p&gt; &lt;ul&gt;&lt;li&gt; &lt;p&gt;Create a new directory in your collection called &lt;code&gt;extensions&lt;/code&gt;.&lt;/p&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the new &lt;code&gt;extensions&lt;/code&gt; directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li&gt; &lt;p&gt;Initialize the default Molecule scenario:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;molecule init scenario&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Edit the &lt;code&gt;molecule.yml&lt;/code&gt; file to use your local collection development environment as described. Add the following entry to your &lt;code&gt;&lt;path_to_your_collection&gt;/extensions/molecule/default/molecule.yml&lt;/code&gt; file:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;provisioner: name: ansible config_options: defaults: collections_path: ${ANSIBLE_COLLECTIONS_PATH}&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;li&gt; &lt;p&gt;Then, set the &lt;code&gt;ANSIBLE_COLLECTIONS_PATH&lt;/code&gt; environment variable at the command line before running Molecule:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;export ANSIBLE_COLLECTIONS_PATH=/home/user/working/collections&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Note that the path should reflect the location up to the &lt;code&gt;collections&lt;/code&gt; directory and not the &lt;code&gt;ansible_collections&lt;/code&gt; directory.&lt;/p&gt; &lt;/li&gt; &lt;/ol&gt;&lt;h2&gt;Molecule scenarios&lt;/h2&gt; &lt;p&gt;Scenarios are the starting point for a lot of powerful functionality that Molecule offers. Think of a scenario as a test suite for roles or playbooks within a collection. You can have as many scenarios as you like, and Molecule will run them sequentially.&lt;/p&gt; &lt;h3&gt;The scenario layout&lt;/h3&gt; &lt;p&gt;Within the &lt;code&gt;molecule/default&lt;/code&gt; folder, we find several files:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;$ ls create.yml destroy.yml molecule.yml converge.yml&lt;/code&gt;&lt;/pre&gt; &lt;ul&gt;&lt;li&gt;&lt;code&gt;create.yml&lt;/code&gt; is a playbook file used for creating the instances and storing data in &lt;code&gt;instance-config&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;destroy.yml&lt;/code&gt; has the Ansible code for destroying the instances and removing them from &lt;code&gt;instance-config&lt;/code&gt;.&lt;/li&gt; &lt;li&gt;&lt;code&gt;molecule.yml&lt;/code&gt; is the central configuration entry point for Molecule per scenario. With this file, you can configure each tool that Molecule will employ when testing your role.&lt;/li&gt; &lt;li&gt;&lt;code&gt;converge.yml&lt;/code&gt; is the playbook file that contains the call for your role. Molecule will invoke this playbook with &lt;code&gt;ansible-playbook&lt;/code&gt; and run it against an instance created by the driver.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Inspecting the molecule.yml&lt;/h3&gt; &lt;p&gt;The &lt;code&gt;molecule.yml&lt;/code&gt; is for configuring Molecule. It is a &lt;a href="https://yaml.org/"&gt;YAML&lt;/a&gt; file with keys that represent the high-level components that Molecule provides. These are:&lt;/p&gt; &lt;ul&gt;&lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#dependency"&gt;dependency&lt;/a&gt; manager:&lt;/strong&gt; Molecule uses &lt;a href="https://docs.ansible.com/ansible/latest/galaxy/dev_guide.html"&gt;galaxy development guide&lt;/a&gt; by default to resolve your role dependencies.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#platforms"&gt;platforms&lt;/a&gt; definitions:&lt;/strong&gt; Molecule relies on this to know which instances to create and name and which group each instance belongs to. If you need to test your role against multiple popular distributions (&lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS&lt;/a&gt;, Fedora, Debian, &lt;a href="https://developers.redhat.com/products/rhel/overview"&gt;Red Hat Enterprise Linux&lt;/a&gt;), you can specify that in this section.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#provisioner"&gt;provisioner&lt;/a&gt;:&lt;/strong&gt; Molecule only provides an Ansible provisioner. Ansible manages the life cycle of the instance based on this configuration.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#scenario"&gt;scenario&lt;/a&gt; definition: &lt;/strong&gt;Molecule relies on this configuration to control the scenario sequence order.&lt;/li&gt; &lt;li&gt;&lt;strong&gt;The &lt;a href="https://ansible.readthedocs.io/projects/molecule/configuration/#verifier"&gt;verifier&lt;/a&gt; framework:&lt;/strong&gt; Molecule uses Ansible by default to provide a way to write specific state-checking tests (such as deployment smoke tests) on the target instance.&lt;/li&gt; &lt;/ul&gt;&lt;h3&gt;Running a full test sequence&lt;/h3&gt; &lt;p&gt;Molecule provides commands to manually manage the lifecycle of the instance, scenario, development, and testing tools. However, we can also tell Molecule to manage this automatically within a scenario sequence.&lt;/p&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the extensions directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The full life cycle sequence can be invoked with &lt;code&gt;molecule test&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;Molecule full lifecycle sequence └── default ├── dependency ├── cleanup ├── destroy ├── syntax ├── create ├── prepare ├── converge ├── idempotence ├── side_effect ├── verify ├── cleanup └── destroy&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Testing the collection role&lt;/h3&gt; &lt;p&gt;One of the default files created as part of the initialization is the &lt;code&gt;converge.yml&lt;/code&gt; file. This file is a playbook created to run your role from start to finish. This can be modified if needed, but is a good place to start if you have never used Molecule before.&lt;/p&gt; &lt;p&gt;You now have an isolated test environment and can also use it for live development by running &lt;code&gt;molecule converge&lt;/code&gt;. It will run through the same steps as above but will stop after the &lt;code&gt;converge&lt;/code&gt; action. Then, you can make changes to your collection or the converge play, and then run &lt;code&gt;molecule converge&lt;/code&gt; again (and again) until you're done with your development work.&lt;/p&gt; &lt;p&gt;We can test the role by adding the following code to &lt;code&gt;converge.yml&lt;/code&gt;:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;--- - name: Include a role from a collection hosts: localhost gather_facts: false tasks: - name: Testing role ansible.builtin.include_role: name: foo.bar.my_role tasks_from: main.yml&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;&lt;code&gt;cd&lt;/code&gt; to the extensions directory:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;cd &lt;path to your collection&gt;/extensions/&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Run the following command:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;molecule converge&lt;/code&gt;&lt;/pre&gt; &lt;div&gt;The above command runs the same steps as the molecule test for the default scenario but will stop after the converge action. This is beneficial if you want to keep the infrastructure up while you are doing your collections development work and testing.&lt;/div&gt; &lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;By introducing Ansible Molecule as a developer preview as part of an Ansible Automation Platform subscription, we are working towards ensuring the project is stable, supported, and maintainable in an enterprise environment.&lt;/p&gt; &lt;p&gt;If you have any questions or feedback on the changes, please reach out to the &lt;a href="https://github.com/ansible/molecule/issues/new/choose"&gt;Ansible Molecule project on Github&lt;/a&gt;. The project's maintainers will be happy to answer any questions on this topic.&lt;/p&gt; &lt;p&gt;As the project matures and evolves, we will keep you updated with more use cases for Ansible Molecule, including deep dives on testing with multiple operating systems, integrating Molecule with your &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;CI/CD&lt;/a&gt; pipelines, and more.&lt;/p&gt; &lt;h2&gt;Where to go next&lt;/h2&gt; &lt;ul&gt;&lt;li&gt;&lt;a href="https://ansible.readthedocs.io/projects/molecule/"&gt;Molecule project documentation&lt;/a&gt;: Check out the detailed documentation on the upstream molecule project that has a lot of getting started use cases with Molecule.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/ansible/getting-started"&gt;Get hands-on with on-demand Ansible Automation Platform self-paced exercises&lt;/a&gt;. We have a variety of interactive in-browser exercises to experience Ansible Automation Platform in action.&lt;/li&gt; &lt;li&gt;&lt;a href="https://developers.redhat.com/products/ansible/download"&gt;Trial subscription&lt;/a&gt;: Are you ready to install on-premises? Get your own trial subscription for unlimited access to all the components of Ansible Automation Platform.&lt;/li&gt; &lt;li&gt;&lt;a href="https://www.youtube.com/ansibleautomation"&gt;Subscribe&lt;/a&gt; to the Red Hat Ansible Automation Platform YouTube channel.&lt;/li&gt; &lt;/ul&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/13/introducing-ansible-molecule-ansible-automation-platform" title="Introducing Ansible Molecule with Ansible Automation Platform"&gt;Introducing Ansible Molecule with Ansible Automation Platform&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Anshul Behl</dc:creator><dc:date>2023-09-13T07:00:00Z</dc:date></entry><entry><title>How Red Hat enhances the developer experience</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/12/how-red-hat-enhances-developer-experience" /><author><name>Mithun T. Dhar</name></author><id>5e063440-428c-4394-98d8-ed28280c925b</id><updated>2023-09-12T18:00:00Z</updated><published>2023-09-12T18:00:00Z</published><summary type="html">&lt;p&gt;Hybrid and multicloud approaches offer developers more access to powerful computing resources than ever. However, this increasing complexity can make it challenging to manage all your development tasks, hindering productivity.&lt;/p&gt; &lt;p&gt;Red Hat's cloud-first approach simplifies modern cloud environments. Our versatile toolbox maintains flexibility and limits cloud vendor lock-in by letting you work with a wide range of cloud tools and vendors. Let's explore how Red Hat reduces friction by designing tools with developers in mind.&lt;/p&gt; &lt;h2&gt;Red Hat reduces friction for developers&lt;/h2&gt; &lt;p&gt;Red Hat's primary goal is to make it easier for you to create and deploy cloud-first applications. Our &lt;a href="https://www.redhat.com/en/topics/cloud/open-hybrid-cloud-approach"&gt;hybrid cloud approach&lt;/a&gt; builds on an &lt;a href="https://developers.redhat.com/topics/open-source-communities"&gt;open source&lt;/a&gt; foundation, enabling you to design software once and deploy it to any (or every) cloud platform.&lt;/p&gt; &lt;p&gt;Our commitment to open source software and cloud development means enabling &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; productivity, providing hosted offerings, and meeting you—the developer—where you are.&lt;/p&gt; &lt;h3&gt;Enabling container productivity&lt;/h3&gt; &lt;p&gt;A considerable portion of simplifying cloud-based development is facilitating container productivity. Central to this effort is &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, a &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt;-based platform that abstracts the intricacies of container management, allowing you to focus on coding instead of dealing with complex infrastructure.&lt;/p&gt; &lt;p&gt;With OpenShift, you can design containerized applications without learning Kubernetes or adjusting your code for specific infrastructure. Choose whether you want to self-manage your applications or have us do it for you. Then, deploy them with a flexible, hybrid-cloud approach, using a single UI to build code and deploy containers, regardless of the underlying infrastructure.&lt;/p&gt; &lt;h3&gt;Meeting developers where they are&lt;/h3&gt; &lt;p&gt;Red Hat's dedication to enhancing the developer experience extends to meeting developers where they're most comfortable. We understand that you have different preferences, so we've created solutions that seamlessly integrate into your existing work patterns.&lt;/p&gt; &lt;p&gt;Our integrated development environments (IDEs) offer code editing, debugging, and version control functions tailored to specific programming languages. The command-line interfaces (CLIs) provide a familiar and efficient way for you to interact with cloud resources and services, accessing the cloud's opportunities without a steep learning curve. Additionally, intuitive web UIs help you manage and monitor applications visually.&lt;/p&gt; &lt;p&gt;By choosing solutions that align with your preferences, you can work in a way that feels intuitive and natural. No need to constantly learn new tools.&lt;/p&gt; &lt;h3&gt;Effortless hosted solutions&lt;/h3&gt; &lt;p&gt;Red Hat provides various hosted offerings where you can experiment, innovate, and iterate seamlessly. Our cloud-hosted setups remove the hassles of provisioning, configuring, and maintaining infrastructure, so you can focus on creating applications.&lt;/p&gt; &lt;p&gt;Red Hat solutions encompass a range of services, from databases to application runtimes, each curated to align with your specific requirements. This approach helps you quickly deploy applications, gather feedback, and iterate rapidly, enabling a dynamic development cycle.&lt;/p&gt; &lt;h2&gt;Designing tools developers want to use&lt;/h2&gt; &lt;p&gt;Red Hat actively designs tools for developers to achieve more in hybrid cloud and multicloud settings.&lt;/p&gt; &lt;p&gt;For example, &lt;a href="https://developers.redhat.com/products/developer-hub/overview"&gt;Red Hat Developer Hub&lt;/a&gt;&lt;strong&gt; &lt;/strong&gt;is a customizable self-managed developer portal. Its self-service dashboard and data aggregation from multiple sources help seamlessly onboard developers and produce containerized applications. The portal includes &lt;a href="https://developers.redhat.com/products/plugins-for-backstage/overview"&gt;Red Hat Plug-ins for Backstage&lt;/a&gt;, a bundle of supported plug-ins for additional functions, ensuring a smoother overall experience.&lt;/p&gt; &lt;p&gt;Another developer tool, &lt;a href="http://developers.redhat.com/articles/2023/05/23/podman-desktop-now-generally-available"&gt;Podman Desktop&lt;/a&gt;, is a lightweight and efficient GUI application helping developers work with containers and Kubernetes from their local environments. Its user-friendly interface is helpful if you're new to containers. You can manage and share containerized applications from your computer, overseeing multiple containers at once without needing to recall complex command-line steps.&lt;/p&gt; &lt;p&gt;If you want to take these tools for a test drive, the &lt;a href="https://developers.redhat.com/developer-sandbox/"&gt;Developer Sandbox for Red Hat OpenShift&lt;/a&gt; is the fastest way to try Red Hat's products and technologies without setup or configuration. In the Developer Sandbox, you can freely experiment with Red Hat OpenShift, its vital developer tools, and cloud services. Guided tutorials and prebuilt sample applications help you learn by example from actual, production-ready services using Red Hat tooling. The Developer Sandbox also integrates with GitHub to import your source code.&lt;/p&gt; &lt;h2&gt;A comprehensive approach to boosting developer productivity&lt;/h2&gt; &lt;p&gt;Red Hat's commitment to the developer experience centers on providing a diverse toolkit so you can achieve your goals in the ever-evolving cloud landscape (Figure 1). Red Hat removes infrastructure complexities via our hosted offerings, simplifying the process of experimenting and iterating.&lt;/p&gt; &lt;p&gt;We seamlessly integrate our platform with IDEs, CLIs, and web UIs to cater to various developer preferences. Plus, Red Hat's emphasis on enabling container productivity, underscored by the OpenShift platform and its tooling, aligns with our dedication to streamlining developer workflows.&lt;/p&gt; &lt;p&gt;This comprehensive approach boosts efficiency while upholding Red Hat's commitment to open hybrid cloud development. Applications can flourish across diverse environments without constraints.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/developer-experience-red-hat.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/developer-experience-red-hat.png?itok=EMrvIwH4" width="600" height="515" alt="A Venn diagram of developer experience, made up of products, programs, and content." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Red Hat's comprehensive approach to developer experience.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;&lt;br /&gt;&lt;a href="https://developers.redhat.com/"&gt;Explore articles and other resources&lt;/a&gt; to learn about how Red Hat helps you do more.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/12/how-red-hat-enhances-developer-experience" title="How Red Hat enhances the developer experience"&gt;How Red Hat enhances the developer experience&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Mithun T. Dhar</dc:creator><dc:date>2023-09-12T18:00:00Z</dc:date></entry><entry><title>A Node.js success story at the electrical training ALLIANCE</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/12/nodejs-success-story-electrical-training-alliance" /><author><name>Michael Dawson</name></author><id>cb489f3b-c2e3-4654-810b-a1d47254da7e</id><updated>2023-09-12T07:00:00Z</updated><published>2023-09-12T07:00:00Z</published><summary type="html">&lt;p&gt;Red Hat and customers often work together as partners to help get an application across the finish line. In this article, Stephen (electrical training ALLIANCE) and Michael (Red Hat) share the story of one such collaboration that led to success with Node.js in production.&lt;/p&gt; &lt;h2&gt;The application&lt;/h2&gt; &lt;p&gt;The &lt;a href="https://electricaltrainingalliance.org/"&gt;electrical training ALLIANCE&lt;/a&gt; (ETA) has the mission to develop educational materials for electrical workers. They do this by developing national standards for the education and training of electrical workers, creating standardized training curricula, and assisting in establishing local educational programs. A key part of this work is to develop and provide an application for the 275 training programs that will assist in the day-to-day operations supporting apprentices within the program from application through to graduation.&lt;/p&gt; &lt;h2&gt;Why Node.js?&lt;/h2&gt; &lt;p&gt;Node.js was a good fit for the training program application due to its natural affinity with the front end and because it allowed faster development and rollout along with streamlined user acceptance testing. &lt;a href="https://access.redhat.com/products/nodejs"&gt;The Red Hat build of Node.js&lt;/a&gt; was a choice that fit into&lt;a href="https://www.redhat.com/en/about/press-releases/electrical-training-alliance-selects-red-hats-managed-cloud-offerings-optimize-it-architecture"&gt; ETA's deployment strategy&lt;/a&gt; as it is Red Hat-maintained, supported, container ready, and runs well in &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;.  &lt;/p&gt; &lt;h2&gt;The journey&lt;/h2&gt; &lt;p&gt;Here we will summarize the process from initial development to deployment.&lt;/p&gt; &lt;h3&gt;Step 1: The CI/CD environment&lt;/h3&gt; &lt;p&gt;Working with Red Hat Consulting, the electrical training ALLIANCE settled on two OpenShift Dedicated clusters: one for non-production and one for production content. Tekton (openshift-pipelines) and Cockroach Cloud were chosen for the &lt;a href="https://developers.redhat.com/topics/ci-cd/"&gt;continuous integration and continuous deployment (CI/CD)&lt;/a&gt; platforms and for data persistence. Application images utilize Red Hat Quay and Red Hat &lt;a href="https://developers.redhat.com/topics/containers"&gt;containers&lt;/a&gt; for end-to-end Red Hat support and patching. These components are shown in Figure 1.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/eta1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/eta1.png?itok=stlF6SB2" width="600" height="393" alt="ETA CI/CD infrastucture based on OpenShift" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Infrastructure components include Tekton and Cockroach Cloud for CI/CD and data persistence and Red Hat Quay and containers for patching and support.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 2: Version 1 of the application using nginx and Node.js&lt;/h3&gt; &lt;p&gt;The project required a front-end and matching back-end for front-end (BFF) to help manage access to a number of existing services written in Quarkus. This is where Node.js came in as a fast and efficient stack on which to build the BFF. Conventional architecture led us to utilize an nginx container for reverse proxy and static asset hosting, along with a Node.js middleware container handling Red Hat OpenShift API Management, authentication, and authorization tasks. This architecture is shown in Figure 2.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node-js-paper-v1.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/node-js-paper-v1.png?itok=mTQwarqQ" width="600" height="429" alt="ETA application V1" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Version 1 of the application with nginx and Node.js.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;The initial version of the application exposed a key challenge due to how nginx must be built as an image (specifically the DNS resolution). There was no way to reuse the image between the non-production and production clusters described earlier, resulting in duplicate build pipelines with no guarantee that a build would succeed in production. This led the electrical training ALLIANCE to explore better options, resulting in version 2 of the application.&lt;/p&gt; &lt;h3&gt;Step 3: Version 2 of the application using Node.js and Next.js&lt;/h3&gt; &lt;p&gt;Since OpenShift provides load balancing, the electrical training ALLIANCE chose to remove nginx and migrate to a Next.js application backed by the Red Hat Node.js container. This allows for proper CI/CD where the image is built once in lower namespaces, tested, and promoted through to the production cluster with confidence.&lt;/p&gt; &lt;p&gt;Looking at existing documentation, however, it was not clear how to use Next.js with the &lt;a href="https://catalog.redhat.com/software/containers/search?q=node.js&amp;p=1"&gt;Red Hat Node.js container images&lt;/a&gt;. After reaching out for help, the electrical training ALLIANCE was pleased to discover that Red Hat had a dedicated &lt;a href="https://docs.google.com/presentation/d/1OVFVU-pWhWm6k5gM59lAI3OjFLOgxVXVECJDv-oSrRY/edit#slide=id.g229b1d01dd4_0_54"&gt;Node.js team&lt;/a&gt; who was happy to jump in and help figure this out. The end result was a solution using a two-stage Dockerfile, which you can read more about in &lt;a href="https://developers.redhat.com/articles/2022/11/23/how-deploy-nextjs-applications-red-hat-openshift"&gt;How to deploy Next.js applications to Red Hat OpenShift&lt;/a&gt;. This work resulted in the architecture shown in Figure 3.&lt;/p&gt; &lt;figure class="align-center rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/node-js-paper-v2.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/node-js-paper-v2.png?itok=248I27Gn" width="600" height="429" alt="ETA Node.js application V2" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 3: Version 2 of the Node.js application with Next.js.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h3&gt;Step 4: Production success&lt;/h3&gt; &lt;p&gt;The next step was production deployment, which is a great success with the applications already having 4,000 active users and potentially growing to 40,000 active users each year as it rolls out across all training locations.&lt;/p&gt; &lt;p&gt;In addition to supporting the initial applications, the Node.js solution was built as a pattern that can be reused for new products and the electrical training ALLIANCE is planning on rolling out many more.&lt;/p&gt; &lt;p&gt;So far, it has been used for the development and production of two products, which include:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;&lt;strong&gt;Training Administration System (TAS):&lt;/strong&gt; This product has a number of functions, including: &lt;ul&gt;&lt;li aria-level="2"&gt;First year online apprenticeship: This allows the electrical training ALLIANCE, partnered with ProTech Skills Institute, to deliver the first year of the apprenticeship's classroom training online. TAS manages the application process for the program, as well as the day-to-day management of the apprentices.&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;strong&gt;Pre-apprenticeship:&lt;/strong&gt; This extends pre-apprenticeship to participating high school and trade schools.&lt;/li&gt; &lt;li aria-level="2"&gt;&lt;strong&gt;VEEP - Veterans to Electrician:&lt;/strong&gt; The &lt;a href="https://in2veep.com/about/"&gt;VEEP program&lt;/a&gt; will utilize TAS for the same purposes while supporting the path to apprenticeship from our veterans.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;&lt;strong&gt;Curriculum analysis and mapping utility: &lt;/strong&gt;This product, developed for internal use, allows the electrical training ALLIANCE to map all of the curriculum against various industry requirements and evaluations to ensure the curriculum provided meets the needs of the industry.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Learn more&lt;/h2&gt; &lt;p&gt;To learn more about the electrical training ALLIANCE, check out &lt;a href="https://electricaltrainingalliance.org"&gt;electricaltrainingalliance.org&lt;/a&gt;. If you want to learn more about what Red Hat is up to on the Node.js front, visit our &lt;a href="https://developers.redhat.com/topics/nodejs"&gt;Node.js page&lt;/a&gt;. &lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/12/nodejs-success-story-electrical-training-alliance" title="A Node.js success story at the electrical training ALLIANCE"&gt;A Node.js success story at the electrical training ALLIANCE&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Michael Dawson</dc:creator><dc:date>2023-09-12T07:00:00Z</dc:date></entry><entry><title>How to trigger jobs manually in Packit</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/08/how-trigger-jobs-manually-packit" /><author><name>Jakub Stejskal, David Kornel</name></author><id>13909259-cf88-4513-a5a4-851cb12e8e99</id><updated>2023-09-08T07:00:00Z</updated><published>2023-09-08T07:00:00Z</published><summary type="html">&lt;p&gt;Packit is an open source project aiming to ease your project's integration with Fedora &lt;a href="https://developers.redhat.com/topics/linux/"&gt;Linux&lt;/a&gt;, &lt;a href="https://developers.redhat.com/products/rhel/centos-and-rhel"&gt;CentOS Stream&lt;/a&gt;, and other distributions. Packit is mostly used by projects that build RPM packages. We won't go through the onboarding process that was already described in a &lt;a href="https://developers.redhat.com/articles/2022/08/16/how-set-packit-simplify-upstream-project-integration#"&gt;previous article&lt;/a&gt;, but we would like to introduce you to new features that were recently promoted into production.&lt;/p&gt; &lt;h2&gt;Testing Farm execution&lt;/h2&gt; &lt;p&gt;From Packit, you can easily trigger the tests on Testing Farm even without building the RPMs. This is very handy for projects that basically don't build RPMs but want to use these two services for verifying the code. As a good example, we can refer to the &lt;a href="https://strimzi.io/"&gt;Strimzi project&lt;/a&gt; where users consume &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; images.&lt;/p&gt; &lt;p&gt;In such cases, the users want to trigger the tests, verify the code and see some output. This option is available from the beginning. Users can easily define when to execute the tests for every pull request, commit, or release. That sounds pretty cool; however, when you have complex tests (5+ hours per test run) as we have in Strimzi, you probably don't want to trigger all tests for each commit. So, how can the users achieve that?&lt;/p&gt; &lt;h2&gt;Manual trigger&lt;/h2&gt; &lt;p&gt;We introduced a new configuration option &lt;code&gt;manual_trigger&lt;/code&gt; to enable triggering Packit jobs only manually. With this new configuration of Packit jobs, users can easily enable the manual trigger of a job, and this job is not automatically triggered when, for example, a new commit arrives to pull a request.&lt;/p&gt; &lt;p&gt;Users just need to specify &lt;code&gt;manual_trigger&lt;/code&gt; in the test's job description. Only boolean values are allowed with the default configuration set to &lt;code&gt;False&lt;/code&gt;. Examples of manual trigger configurations can be found in the &lt;a href="https://github.com/strimzi/strimzi-kafka-operator/blob/main/.packit.yaml"&gt;YAML file&lt;/a&gt;.&lt;/p&gt; &lt;pre&gt; ... - job: tests trigger: pull_request identifier: "regression-operators" targets: - centos-stream-9-x86_64 - centos-stream-9-aarch64 skip_build: true manual_trigger: true labels: - regression - operators - regression-operators - ro tf_extra_params: test: fmf: name: regression-operators ... &lt;/pre&gt; &lt;p&gt;This new configuration allows users to onboard a new flow when a pull request is opened. For example, in draft mode, users push new commits and fixes, and when they are about to finish the pull request, they can easily type &lt;code&gt;/packit test&lt;/code&gt; as a pull request comment, and all jobs defined in &lt;code&gt;packit.yaml&lt;/code&gt; for the pull request are triggered.&lt;/p&gt; &lt;h2&gt;Labeling and identifying&lt;/h2&gt; &lt;p&gt;The solution just described is very easy to use; however, there might be use cases where the users don't want to trigger all the jobs. For example, when you have 10 jobs defined with different test scopes, you probably don't want to trigger acceptance and regression tests at the same time as acceptance could be a subset of regression.&lt;/p&gt; &lt;p&gt;Users now have two options for triggering a specific job. The first one is to trigger the job based on the identifier. When the user specifies &lt;code&gt;identifier: test-1&lt;/code&gt; in the job configuration, the Packit comment command for execution of the tests will look like this &lt;code&gt;/packit test –identifier test1&lt;/code&gt;. That command will execute jobs with this specific identifier, nothing else (Figure 1).&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-22_at_11.27.52.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-22_at_11.27.52.png?itok=ofWtbNJ4" width="600" height="506" alt="Triggering jobs based on a specific identifier." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 1: Packit manual trigger 1.&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;What if we want to execute more than one job? Users can use multiple identifiers in a comma-separated list, but it might be a little bit annoying to specify long identifiers every time. To add a better user experience, we've introduced &lt;code&gt;labels&lt;/code&gt; configuration that could group together multiple jobs. Command &lt;code&gt;/packit test –labels upgrade,regression&lt;/code&gt; will trigger all jobs that contain &lt;code&gt;upgrade&lt;/code&gt; or &lt;code&gt;regression&lt;/code&gt; in the list of labels in the job configuration.&lt;/p&gt; &lt;figure class="rhd-u-has-filter-caption" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_2023-06-22_at_11.30.38.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_2023-06-22_at_11.30.38.png?itok=jBRlKw9l" width="600" height="506" alt="Triggering a group of jobs via the label configuration." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;Figure 2: Packit manual trigger 2.&lt;/figcaption&gt;&lt;/figure&gt;&lt;h2&gt;Conclusion&lt;/h2&gt; &lt;p&gt;If you hesitated with onboarding to Packit due to the limitation of missing manual triggering of the jobs and missing labeling, you can start with onboarding now! As we already mentioned, Packit is an &lt;a href="https://devconfcz2023.sched.com/event/1MYme/become-an-open-source-service?linkback=grid"&gt;open source service&lt;/a&gt; and these improvements were done as contributions from outside of the Packit team. Everyone can contribute, so if you are missing some features, feel free to open a pull request.&lt;/p&gt; &lt;p&gt;To see more information about newly added options, check out the &lt;a href="https://packit.dev/docs/testing-farm/"&gt;documentation&lt;/a&gt;. If you are new to Packit, you can also watch talks from the Packit team from &lt;a href="https://devconfcz2023.sched.com/event/1MYlL/packit-rpm-integration-all-in-one?linkback=grid"&gt;DevConf 2023&lt;/a&gt; and &lt;a href="https://www.youtube.com/watch?v=e2aCilMy-5U"&gt;DevConf Mini 2023&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/08/how-trigger-jobs-manually-packit" title="How to trigger jobs manually in Packit"&gt;How to trigger jobs manually in Packit&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Jakub Stejskal, David Kornel</dc:creator><dc:date>2023-09-08T07:00:00Z</dc:date></entry><entry><title>Drop git pull for fetch and rebase</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/07/drop-git-pull-fetch-and-rebase" /><author><name>Yftach Herzog</name></author><id>f06cdd57-aa99-482f-834b-86238f0e8672</id><updated>2023-09-07T07:00:00Z</updated><published>2023-09-07T07:00:00Z</published><summary type="html">&lt;p&gt;I would like to explain why the &lt;code&gt;git pull&lt;/code&gt; command is not to be used lightly and to question whether it is ever needed. The &lt;code&gt;git pull&lt;/code&gt; command may look harmless, but it is used in ways that often leave a fair amount of mess. I will discuss safer alternatives. This article is for beginner to intermediate Git users looking to extend their skills in using pull requests and merge requests when collaborating on a project.&lt;/p&gt; &lt;h2&gt;Alternatives to git pull&lt;/h2&gt; &lt;p&gt;This section provides a condensed version of an approach for contributing to a software project without using &lt;code&gt;git pull&lt;/code&gt;. I will go into more detail later.&lt;/p&gt; &lt;p&gt;Configure two remotes on your local repository so that you have &lt;code&gt;origin&lt;/code&gt; pointing to your fork and &lt;code&gt;upstream&lt;/code&gt; pointing to the repository you’re contributing to as follows:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git clone forked-repo-url → cd repo-name → git remote add upstream upstream-repo-url → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push) upstream upstream-repo-url (fetch) upstream upstream-repo-url (push) &lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Forget about your fork’s main branch. There is no reason to keep your main branch in sync with the upstream repository’s main branch. It’s a maintenance burden that serves no purpose. You should only maintain branches that are part of an ongoing PR work. The rest can be deleted.&lt;/p&gt; &lt;h3&gt;Starting pull request work&lt;/h3&gt; &lt;p&gt;Create your PR branch directly from the upstream repository’s branch it should be merged to (typically main):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout -b my-pr-branch upstream/main → git add new-file some-other-file → git commit → git push origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Syncing with ongoing work&lt;/h3&gt; &lt;p&gt;Use &lt;code&gt;rebase&lt;/code&gt; when you need to have your PR branch synchronized with changes on the target branch (address conflicts as needed):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout my-pr-branch → git rebase upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;h3&gt;Don’t add unnecessary commits&lt;/h3&gt; &lt;p&gt;Don’t create new commits throughout the PR progression. Try limiting your PR to a single commit and add later changes by amending the original commit. Use force push to update the remote branch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file → git commit --amend → git push --force origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;In other words, use &lt;code&gt;git commit&lt;/code&gt; (without &lt;code&gt;--amend&lt;/code&gt;) only for the first time you create the PR’s commit(s). Later on, only use &lt;code&gt;git commit --amend&lt;/code&gt;.&lt;/p&gt; &lt;h2&gt;Typical change workflow&lt;/h2&gt; &lt;p&gt;Let’s assume this is a scenario in which developers contribute code to a repository. We’ll call it the upstream repository, to which they don’t necessarily have write access.&lt;/p&gt; &lt;p&gt;To contribute code, developers will do the following:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Fork the upstream repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Clone their forked repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a feature branch out of the main branch of their forked repository (assuming its name is main, but it can be any other name).&lt;/li&gt; &lt;li aria-level="1"&gt;Introduce the code changes locally, commit, and push them to the newly-created feature branch on the forked repository.&lt;/li&gt; &lt;li aria-level="1"&gt;Create a pull request on GitHub (or a merge request on GitLab).&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;At this point, developers will ask their peers to review their code, address peers' comments, and push the changes to their forked repository in order to have their PR approved and merged.&lt;/p&gt; &lt;p&gt;But what happens when the upstream repository progresses? There are times when we need our feature branch to include the latest changes from the target branch (the upstream repository’s main branch). This might be because of conflicts between our work and the upstream repository, or maybe we have automated tests that have to run on an up-to-date feature branch to verify that we didn't introduce regressions.&lt;/p&gt; &lt;p&gt;When we need to get in sync with the upstream repository, a simpler scenario is where we want to create another PR to the same repository, but we don’t have the latest progress made on the upstream repository.&lt;/p&gt; &lt;h2&gt;Using git pull is risky&lt;/h2&gt; &lt;p&gt;How do I get all that upstream progress to my PR branch? At this point, one might say &lt;code&gt;git pull&lt;/code&gt; must be the opposite of &lt;code&gt;git push&lt;/code&gt;, so let’s use it to update my stuff with the upstream changes.&lt;/p&gt; &lt;p&gt;But &lt;code&gt;git pull&lt;/code&gt; is the opposite of &lt;code&gt;git push&lt;/code&gt; only in very specific cases. That is, when the local checked out branch can be fast-forwarded to the state of the branch being pulled.&lt;/p&gt; &lt;p&gt;When we want to push commits to an existing remote branch, &lt;code&gt;git push&lt;/code&gt; will only go through if the remote branch did not diverge from our local branch. It only contains commits that exist on the local branch. If the remote branch contains commits not on the local branch, &lt;code&gt;git push&lt;/code&gt; will fail.&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git push origin HEAD To /tmp/tmp.B2Ljc86u9L ! [rejected] HEAD -&gt; foo (non-fast-forward) error: failed to push some refs to '/tmp/tmp.B2Ljc86u9L' hint: Updates were rejected because the tip of your current branch is behind hint: its remote counterpart. Integrate the remote changes (e.g. hint: 'git pull ...') before pushing again. hint: See the 'Note about fast-forwards' in 'git push --help' for details.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This is not the same for &lt;code&gt;git pull&lt;/code&gt;. The &lt;code&gt;git pull&lt;/code&gt; command performs &lt;code&gt;git fetch&lt;/code&gt; and then &lt;code&gt;git merge&lt;/code&gt; (this is configurable, but those are the typical defaults).&lt;/p&gt; &lt;p&gt;The &lt;code&gt;git fetch&lt;/code&gt; command will update the remote-tracking branches (local branches mirroring remote branches), which is harmless.&lt;/p&gt; &lt;p&gt;The &lt;code&gt;git merge&lt;/code&gt; command will merge the changes on the remote-tracking branch to the local branch.&lt;/p&gt; &lt;p&gt;This has some drawbacks:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;If those changes cannot be fast-forwarded, it means a merge-commit will be created on the local branch. &lt;ul&gt;&lt;li aria-level="2"&gt;If that local branch is our main branch, this is probably not what we want.&lt;/li&gt; &lt;li aria-level="2"&gt;If that local branch is a PR branch, it means that our PR will now include a merge-commit, which is confusing for reviewers and makes our history look ugly.&lt;/li&gt; &lt;/ul&gt;&lt;/li&gt; &lt;li aria-level="1"&gt;Which remote branch is actually going to be merged into our local branch? We can control that, but we cannot assume git will necessarily be smart about picking the right one.&lt;/li&gt; &lt;/ul&gt;&lt;p&gt;All in all, using &lt;code&gt;git pull&lt;/code&gt;, puts us at risk of turning our PR branch (and the upstream branch if the changes are merged) into a merge-commits spaghetti or even merging changes from unexpected remote branches into our PR branch.&lt;/p&gt; &lt;h2&gt;Take control&lt;/h2&gt; &lt;p&gt;Git is powerful. Which means harmful actions can easily happen. We should be even more cautious with using git shortcuts embedded into our IDEs, graphical git utilities, and nice-looking buttons on GitHub that are supposed to solve our issues with a click.&lt;/p&gt; &lt;p&gt;At least some of those shortcuts can't read our minds yet. They will not do what we expect them to do, especially if we don’t know what we want them to do. They surely won’t clean up the mess they made.&lt;/p&gt; &lt;h3&gt;Do we need so many branches?&lt;/h3&gt; &lt;p&gt;When we navigate to our fork in GitHub, it usually warns us that our main branch is many commits behind the upstream branch (Figure 1). This makes us think that our main branch should be in sync with the upstream branch.&lt;/p&gt; &lt;figure class="align-center" role="group"&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/gazillion-commits-behind.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/gazillion-commits-behind.png?itok=veFTgIEF" width="600" height="47" alt="A screenshot of a GitHub warning for a branch that is many commits behind the upstream branch." typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: GitHub warns that our branch is many commits behind the target branch.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;figcaption class="rhd-c-caption"&gt;&lt;/figcaption&gt;&lt;/figure&gt;&lt;p&gt;But does this branch really serve any purpose?&lt;/p&gt; &lt;p&gt;Let’s assume our fork exists only as a means to contribute code to an upstream repository rather than to develop a spin-off, which is usually the case.&lt;br /&gt; I would argue that our fork’s main branch should not be a part of our contribution workflow. This is because a PR is a proposal for merging a feature branch to an upstream branch. Our fork’s main branch has nothing to do with that.&lt;/p&gt; &lt;p&gt;In other words, we start our PR branches out of the target upstream branch (e.g., the upstream main branch, not our fork’s main branch). In case we need to get our PR branch up-to-speed with current changes, it’s the target upstream branch we need to sync with, not our fork’s main branch.&lt;/p&gt; &lt;p&gt;Synchronizing our fork’s main branch with its upstream counterpart serves no purpose. We don’t need it, and performing pointless tasks is just another opportunity to introduce mistakes and mess up our environments.&lt;/p&gt; &lt;h3&gt;Starting at the right point&lt;/h3&gt; &lt;p&gt;So, how do we start a new PR branch from the tip of our upstream branch? Simple, we fetch our remote-tracking branches for the upstream repository. To do that, we first need to have a remote defined for the upstream repository.&lt;/p&gt; &lt;p&gt;Let’s assume we created our local repository by cloning our fork using the default options. Something like this:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git clone forked-repo-url&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;By default, we should now have a remote called &lt;code&gt;origin&lt;/code&gt; pointing to the forked repo defined under the local repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cd repo-name → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will allow us to push changes to our fork.&lt;/p&gt; &lt;p&gt;In order to get the latest changes from the upstream repository, we need to add a remote for that repository:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git remote add upstream upstream-repo-url → git remote -v origin forked-repo-url (fetch) origin forked-repo-url (push) upstream upstream-repo-url (fetch) upstream upstream-repo-url (push)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;We now have the remote defined. To synchronize the remote-tracking branches, we need to fetch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;With that, we create a remote-tracking branch called &lt;code&gt;upstream/main&lt;/code&gt; (assuming that’s the branch for which we want to create a PR). This is a local branch containing the content of the main branch on the upstream repository at the time we last fetched upstream.&lt;/p&gt; &lt;p&gt;We can list our remote-tracking branches:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git branch --remote origin/HEAD -&gt; origin/main upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;To create our feature branch at the current state of &lt;code&gt;upstream/main&lt;/code&gt;, we use:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git checkout -b my-pr-branch upstream/main&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This command creates a new branch called &lt;code&gt;my-pr-branch&lt;/code&gt;at the commit &lt;code&gt;upstream/main&lt;/code&gt; points to and switches to the newly-created branch.&lt;/p&gt; &lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;About PR progression&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;We should now make our changes, &lt;code&gt;git add&lt;/code&gt; them, commit, and push them to origin:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file → git commit → git push origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;This will push our changes to &lt;code&gt;origin&lt;/code&gt; (the remote pointing to our fork) into a branch with the same name as our local branch (the &lt;code&gt;HEAD&lt;/code&gt; keyword points to the latest commit on the currently checked-out branch).&lt;/p&gt; &lt;p&gt;We will now use the GitHub UI to create a PR. We will make sure that the source branch of the PR is the newly-created feature branch on our fork, while the target branch is the branch to which we want to propose changes (in this case, the main branch of the upstream repository).&lt;/p&gt; &lt;p&gt;Our diligent peers will thoroughly review our work and point us to some issues requiring our attention.&lt;/p&gt; &lt;p&gt;We will then fix those issues locally and &lt;code&gt;git add&lt;/code&gt; them, but we will not include them on a new commit. We will instead use them to amend the original commit and then force-push them to the same branch:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add changed-file another-file some-other-file → git commit --amend → git push --force origin HEAD&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The reason for amending the commit rather than creating a new one is so that our PR’s commits will ultimately represent the progression of the code we propose to introduce to the upstream branch rather than representing the progression of the PR work.&lt;/p&gt; &lt;p&gt;In other words, if our PR intends to fix the fairy dust dispenser, then we want it to include a commit with title “fix the fairy dust dispenser” which will contain all changes required for fixing the fairy dust dispenser, rather than 3 commits titled “fix the fairy dust dispenser”, “removing prints”, “addressing comments”.&lt;/p&gt; &lt;p&gt;By amending the commit, we’re diverging from the remote branch. We created a new commit instead of the one we already pushed. So Git will not allow us to push to it. The remote branch contains our original commit, which our local branch doesn't have anymore.&lt;/p&gt; &lt;p&gt;At this point, it will even suggest that we use git pull to fix it (see the previous rejection message). Don’t use git pull for that. It will not fix our issue and will create others instead. Don’t use git pull at all.&lt;/p&gt; &lt;p&gt;We now know a few things about Git, and we have some confidence in what we’re doing. So we force-push instead, telling Git that we want to replace the content of the remote branch with the content of the local branch. That’s the reason for using &lt;code&gt;push --force&lt;/code&gt;.&lt;/p&gt; &lt;h3&gt;Fetch and rebase to the rescue&lt;/h3&gt; &lt;p&gt;It might also be that while we were busy waiting for reviews, some other changes merged to the target branch on the upstream repository. In this case, we might need to get in sync with that target branch.&lt;/p&gt; &lt;p&gt;Will it be &lt;code&gt;git pull&lt;/code&gt; to the rescue? No. By default, &lt;code&gt;git pull&lt;/code&gt; will create a merge commit on our feature branch, merging the work done on the target branch since we started working on it. This will make reviewers’ lives harder and, if merged, will not look nice on the target branch history (e.g., how would it look if we try to revert this PR for some reason at a later stage?).&lt;/p&gt; &lt;p&gt;Or will it be &lt;code&gt;git fetch&lt;/code&gt; and &lt;code&gt;git rebase&lt;/code&gt; to the rescue? Indeed!&lt;/p&gt; &lt;p&gt;To overcome this, we must replay our changes on top of the latest changes on the target branch. To do that, we need to synchronize our remote-tracking branch with the upstream repository and rebase our PR branch on top of it:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch upstream → git checkout my-pr-branch → git rebase upstream/main Successfully rebased and updated refs/heads/my-pr-branch.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;The &lt;code&gt;git rebase&lt;/code&gt; command will find the commit that is the common ancestor of our PR branch and the target branch. That should be the commit from which we started our work. It will take all the commits on our PR branch introduced after that point and replay them on top of the target branch.&lt;/p&gt; &lt;p&gt;Suppose that we started our work when the target branch's Git history (git log) looked something like this (newest commits first):&lt;/p&gt; &lt;pre&gt; &lt;code&gt;commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;On our PR branch, we created a commit for the content we want to deliver, so its history looks something like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;our pr commit commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Our busy colleagues did some work in the meantime, and the target branch now looks like this:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;yet more work done while our pr was in review some work done while our pr was in review commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If we fetch the upstream repo, we now have the remote-tracking branch &lt;code&gt;upstream/main&lt;/code&gt; containing this history.&lt;/p&gt; &lt;p&gt;If we checkout our PR branch and rebase on top of &lt;code&gt;upstream/main&lt;/code&gt;, Git will:&lt;/p&gt; &lt;ol&gt;&lt;li aria-level="1"&gt;Find the newest commit existing on both branches, which is the one named "commit happened just before…".&lt;/li&gt; &lt;li aria-level="1"&gt;Take all commits on our branch that happened after that point. In this case, it’s the one named "our pr commit".&lt;/li&gt; &lt;li aria-level="1"&gt;Reset our PR branch to the current state of the remote-tracking branch.&lt;/li&gt; &lt;li aria-level="1"&gt;Replay "our pr commit" on top of that.&lt;/li&gt; &lt;/ol&gt;&lt;p&gt;The result:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;our pr commit yet more work done while our pr was on review some work done while our pr was on review commit happened just before we started working on our PR slightly older commit even older commit&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Replaying our changes on top of the latest changes means that it will create new commit(s) with the same changes from our original commit(s). Namely, they will look the same in terms of the changes they made, but they will have different commit hash because the starting point for the changes is different.&lt;/p&gt; &lt;h3&gt;Rebase looks simple enough&lt;/h3&gt; &lt;p&gt;Rebasing replays the original changes we made on top of the current state of the branch on which we rebase upon. What happens if the starting point of the content we changed is not the same anymore? In other words, what happens if the lines that we changed in the PR's commit(s) were also changed by the commits that were added to the target branch in the meantime?&lt;/p&gt; &lt;p&gt;The answer is: conflicts.&lt;/p&gt; &lt;p&gt;In the following example, we’re trying to rebase branch &lt;code&gt;foobar&lt;/code&gt; with a commit message "add bar" on top of the &lt;code&gt;upstream/main&lt;/code&gt; branch having at its tip a commit called "add baz", which does not exist on the PR branch. The two commits are changing the same line on a file called foo.&lt;/p&gt; &lt;p&gt;In this case, instead of telling us that our branch was successfully rebased, Git will let us know which files it failed to process:&lt;/p&gt; &lt;pre&gt; &lt;code&gt;Auto-merging foo CONFLICT (content): Merge conflict in foo error: could not apply 7a384ae... add bar Resolve all conflicts manually, mark them as resolved with "git add/rm &lt;conflicted_files&gt;", then run "git rebase --continue". You can instead skip this commit: run "git rebase --skip". To abort and get back to the state before "git rebase", run "git rebase --abort". Could not apply 7a384ae... add bar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It will also add markers within the failed files denoting the conflict(s):&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cat foo &lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD baz ======= bar &gt;&gt;&gt;&gt;&gt;&gt;&gt; 7a384ae (add bar)&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;It could be a bit confusing, but everything between the &lt;&lt;&lt; markers and the === markers is the content coming from the branch on top of which we’re trying to rebase (the target branch), while the stuff between the === markers and the &gt;&gt;&gt; markers is the content coming from the branch we try to place at the top (the changes coming from our PR branch).&lt;/p&gt; &lt;p&gt;What we now need to do is to decide what should be the correct content for each conflict and then delete all markers. In our case, we’re going to delete the markers and add the conflicting statements under the same line:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ cat foo baz bar&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;If we try to rebase multiple commits, there may be conflicts on each and every commit, and we’d need to decide what should have been the content of each conflicting line for each such conflict at the point in which each commit is applied.&lt;/p&gt; &lt;p&gt;Sounds complicated. Which is yet another reason to make small PRs including only a single commit (easier code review is another reason).&lt;/p&gt; &lt;p&gt;Once we resolve all conflicts, we need to &lt;code&gt;git add&lt;/code&gt; all files that had conflicts, and run &lt;code&gt;git rebase --continue&lt;/code&gt;. A common mistake is committing the changes instead.&lt;/p&gt; &lt;p&gt;Do not use &lt;code&gt;git commit&lt;/code&gt; during a rebase process, as we’re not trying to add a new commit, just to fix conflicts on existing commits:&lt;/p&gt; &lt;pre&gt; &lt;code class="language-bash"&gt;→ git add foo → git rebase --continue ... Successfully rebased and updated refs/heads/foobar.&lt;/code&gt;&lt;/pre&gt; &lt;p&gt;Now you can force-push the changes and later repeat the same steps if you need to attend to more issues until your code is ready to be merged.&lt;/p&gt; &lt;h3&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;Tips to track your progress&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/h3&gt; &lt;p&gt;It is easy to lose track of your current step, so follow these tips.&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;Use &lt;code&gt;git status&lt;/code&gt; to see your staged and unstaged changes and the branch you have checked out.&lt;/li&gt; &lt;li aria-level="1"&gt;Use &lt;code&gt;git log&lt;/code&gt; to convince yourself that your Git history makes sense. Each commit line includes all branches pointing to this commit. After rebasing on top of a branch, you should see your commit(s) at the top, and the target branch just underneath. If that is not the case, then we need to figure out what we did wrong.&lt;br /&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;In this example, we’re on branch foobar, and we rebased the “add bar” commit on top of &lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;upstream/main&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;&lt;span&gt;:&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt;&lt;/span&gt; &lt;pre&gt; &lt;code&gt;af2e399 (HEAD -&gt; foobar) add bar 5ba3bf7 (upstream/main) add baz 3b3b1bb add foo&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;At any step before the rebase process is done, you can abort it and revert to the stage before the process started with &lt;code&gt;git rebase --abort&lt;/code&gt;.&lt;/li&gt; &lt;li aria-level="1"&gt;If you realize you made a mistake, there are still a few ways to go back. One such option is to override your local branch with the last version you pushed to the remote (losing all local progress): &lt;pre&gt; &lt;code class="language-bash"&gt;→ git fetch origin → git checkout my-pr-branch → git reset --hard origin/my-pr-branch&lt;/code&gt;&lt;/pre&gt; &lt;/li&gt; &lt;/ul&gt;&lt;ul&gt;&lt;li aria-level="1"&gt;Use &lt;code&gt;git show&lt;/code&gt; before you push your changes to the remote to show the content of your last commit in order to convince yourself it makes sense. Until you push your changes, you still have the remote branch as backup.&lt;/li&gt; &lt;/ul&gt;&lt;h2&gt;Summary&lt;/h2&gt; &lt;p&gt;Contrary to this somewhat opinionated text, I honestly believe that everyone should do what works for them. In my opinion, using &lt;code&gt;git pull&lt;/code&gt; is problematic, and I have explained why and offered alternatives. What is important to keep in mind is that while Git is powerful, it is not forgiving. You can use &lt;code&gt;git reflog&lt;/code&gt; to undo many mistakes, but it’s not easy to use. For this reason, when we do something, we need to know the expected result, such as which branch or remote will be affected and what is changing. Hopefully, you will also know how the chosen steps will take you there.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/07/drop-git-pull-fetch-and-rebase" title="Drop git pull for fetch and rebase"&gt;Drop git pull for fetch and rebase&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Yftach Herzog</dc:creator><dc:date>2023-09-07T07:00:00Z</dc:date></entry><entry><title>Quarkus extensions give Java dependencies superpowers</title><link rel="alternate" href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" /><author><name>Kevin Dubois</name></author><id>1865d737-c8ff-415b-9ed7-1b4b8fc7678a</id><updated>2023-09-06T07:00:00Z</updated><published>2023-09-06T07:00:00Z</published><summary type="html">&lt;p&gt;&lt;a href="https://developers.redhat.com/products/quarkus"&gt;Quarkus&lt;/a&gt; extensions are one of Quarkus' best hidden-in-plain-sight features. Read on to learn how Quarkus extensions give &lt;a href="https://developers.redhat.com/java"&gt;Java&lt;/a&gt; superpowers and how you can get started with them.  &lt;/p&gt; &lt;h2&gt;What are Quarkus extensions?&lt;/h2&gt; &lt;p&gt;Quarkus extensions are essentially adapter layers for Java-based libraries or technologies that enhance your application.&lt;/p&gt; &lt;p&gt;However, the scope of Quarkus extensions goes well beyond "just" importing dependent libraries. They can significantly increase the application's performance, help developers be more productive while developing their applications, integrate complex dependencies much easier, and simplify the application's source code.&lt;/p&gt; &lt;p&gt;Examples of Quarkus extensions include the Java Database Connectivity (JDBC) libraries, OpenAPI generators, &lt;a href="https://developers.redhat.com/topics/kubernetes"&gt;Kubernetes&lt;/a&gt; manifest generators, and Apache Camel components. But there are many, many more.  &lt;/p&gt; &lt;h2&gt;Quarkus extensions enhance app performance&lt;/h2&gt; &lt;p&gt;A significant advantage of using Quarkus extensions is that they integrate seamlessly into the Quarkus architecture to take advantage of its superb build time optimization phase. This way, extensions can prescribe how to load and scan your application's bytecode (including the dependencies) and configuration optimally during the build augmentation stage instead of during startup time. This allows for significantly reduced resource usage and a much faster startup time for the application's dependent libraries and technologies, just like the core Quarkus components.&lt;/p&gt; &lt;p&gt;Extensions also get access to the preparation phase for GraalVM native compilation so that they can leverage the advantages of natively compiled Quarkus binaries. Keep in mind that it is the extension's responsibility to make sure it is compatible with native compilation, so make sure to read its documentation. For a deeper understanding of how this works, check out the &lt;a href="https://quarkus.io/guides/writing-extensions#technical-aspect"&gt;Quarkus documentation on how to write an extension&lt;/a&gt;.&lt;/p&gt; &lt;h2&gt;Quarkus extensions increase developer productivity and joy&lt;/h2&gt; &lt;p&gt;Aside from aiding the performance of your application, Quarkus extensions can give a significant boost to developer productivity and help make developing applications more enjoyable as well by not having to fiddle around with configurations, extend the Quarkus CLI, and being able to leverage Quarkus's Dev Mode.&lt;/p&gt; &lt;h3&gt;Dev Mode&lt;/h3&gt; &lt;p&gt;Quarkus Dev Mode is a feature that starts up the application locally (or even remotely) and provides a set of capabilities to allow developers to iterate and test their code changes quickly. Dev Mode does targeted hot reloads on the fly when you change your code. This has the benefit of not needing to do manual restarts or rebuilding the application each time you change something. &lt;/p&gt; &lt;p&gt;This benefit applies to extensions as well. They will only be (re-)loaded if they have been changed, added, or removed. Any dependent configuration of the extension, such as a database migration script or similar, will also have the same behavior. For example, if you include an &lt;code&gt;import.sql&lt;/code&gt;, the import statement will run only during Dev Mode startup or when the file gets changed.&lt;/p&gt; &lt;h3&gt;Dev Services&lt;/h3&gt; &lt;p&gt;Extensions can also leverage the &lt;a href="https://quarkus.io/guides/dev-services"&gt;Quarkus Dev Services&lt;/a&gt; capability. Dev Services provides an automated way to spin up dependent services, typically in a local &lt;a href="https://developers.redhat.com/topics/containers"&gt;container&lt;/a&gt; and usually using the &lt;a href="https://www.testcontainers.org/"&gt;Testcontainers&lt;/a&gt; project. Think of databases, Apache Kafka clusters, Keycloak, etc. Dev Services are wired into your application without further configuration, making them a powerful way to increase developer productivity because you don't have to worry about setting up complicated dependent services on your local machine.&lt;/p&gt; &lt;p&gt;To use Dev Services, all you have to do is add the relevant extension—e.g. &lt;code&gt;jdbc-postgresql&lt;/code&gt; or &lt;code&gt;camel-quarkus-kafka&lt;/code&gt; (Figure 1)—and when you start up Quarkus's &lt;a href="https://quarkus.io/guides/dev-mode-differences#dev-mode-features"&gt;Dev Mode&lt;/a&gt; (using the command &lt;code&gt;quarkus dev&lt;/code&gt; or &lt;code&gt;mvn quarkus:dev&lt;/code&gt;), the extension will start up a Dev Service automatically. Because Dev Services usually rely on containers, you will need to have a container runtime on your machine such as Podman or Docker to enjoy this feature (Figure 2).&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_15-39-19.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_15-39-19.png?itok=nk0JGQ38" width="600" height="84" alt="adding the camel-quarkus-kafka extensions automatically starts up a Kafka Dev Service" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 1: Adding the camel-quarkus-kafka extension automatically starts a Kafka Dev Service during Dev Mode.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_08-55-32.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_08-55-32.png?itok=bPoYfBjO" width="600" height="310" alt="Kafka Dev Service running in Podman Desktop" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 2: The running Kafka Dev Services container (using an image provided by Redpanda) shown in Podman Desktop&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h3&gt;Dev UI&lt;/h3&gt; &lt;p&gt;When Quarkus runs in Dev Mode, it also serves a web application called the Dev UI, which, among other things, shows the extensions currently used by your application. Extensions can expose additional information and capabilities in the Dev UI. At the minimum, they show details of the extension or a link to further documentation (which can be very handy!).&lt;/p&gt; &lt;p&gt;Extensions can add actionable capabilities to the Dev UI as well (Figure 3). For many extensions, you can change the configuration on the fly through the Dev UI, and some extensions have purpose-built capabilities such as building a container image, deploying to &lt;a href="https://developers.redhat.com/products/openshift/overview"&gt;Red Hat OpenShift&lt;/a&gt;, accessing and interacting with the Swagger UI, or even adding records to a Kafka topic running in a Dev Service. Of course, these capabilities depend on the extension and what has been implemented for it.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-11_15-58-18.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-11_15-58-18.png?itok=VluaeRxq" width="600" height="240" alt="Quarkus Dev UI" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 3: Sample extensions shown in the Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;p&gt;Extensions with a related Dev Service will also display the configuration that was automatically generated by Quarkus when the Dev Service started up. This can be very useful for developers to determine what configuration settings are needed for a particular dependency, such as the data source's type, JDBC URL, username, and password, as you can see in Figure 4.&lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-14_11-41-05.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-14_11-41-05.png?itok=NctIAfQ6" width="600" height="426" alt="Quarkus Dev UI showing Kafka and Postgresql Dev Services" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 4: Dev Services shown in the Quarkus Dev UI.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus CLI&lt;/h2&gt; &lt;p&gt;The &lt;code&gt;quarkus&lt;/code&gt; command lets you create projects, manage extensions, and do essential build and development tasks using the underlying project build tool (e.g., Maven or Gradle). Extensions can extend the functionality of the Quarkus CLI commands by virtue of Quarkus CLI plug-ins (Figure 5). This plug-in system can dynamically add commands and subcommands to the CLI. Some plug-in extensions are available out of the box, which you can retrieve with the Quarkus &lt;code&gt;plug-in list-installable&lt;/code&gt; command. In addition, you can also install executable jars from community extensions using their Maven coordinates. &lt;/p&gt; &lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-21_14-08-29.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-21_14-08-29.png?itok=S-kbeDfo" width="600" height="104" alt="List of Quarkus CLI plugins" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 5: An example list of available Quarkus CLI plug-ins.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Quarkus extensions catalog and versioning&lt;/h2&gt; &lt;p&gt;Quarkus has an extensive catalog of official Quarkus extensions. To get an idea of the extensions that are available for Quarkus, take a look at the &lt;a href="https://quarkus.io/extensions/"&gt;quarkus.io extensions catalog page&lt;/a&gt;.&lt;/p&gt; &lt;p&gt;There is no need to keep track of the versioning of any individual extension, thanks to the concept of the Quarkus platform. This concept promises that any combination of the Quarkus extensions within the platform can be used in the same application without causing conflict. This also means that the versions of the extensions are determined by the platform. In practice, this works through the use of a Quarkus platform BOM artifact, which is imported as a dependency on your project.&lt;/p&gt; &lt;p&gt;Thanks to this Quarkus platform concept, you don't have to explicitly define the version of each Quarkus extension (or the libraries they are acting as a wrapper for) in your &lt;code&gt;pom.xml&lt;/code&gt; or &lt;code&gt;build.gradle&lt;/code&gt; file, and when you upgrade your Quarkus version, your extension versions are also automatically updated. Bear in mind that the extensions catalog might also contain extensions that are not necessarily production-ready, so it's always a good practice to read up on the extension's documentation before using it.&lt;/p&gt; &lt;h2&gt;The Quarkiverse&lt;/h2&gt; &lt;p&gt;The &lt;a href="http://github.com/quarkiverse"&gt;Quarkiverse GitHub organization&lt;/a&gt; provides hosting for Quarkus extension projects—not only for the ones that are featured in the main Quarkus extensions catalog, but also for additional extensions provided by Quarkus community developers. These community extensions are fully maintained by the extension's independent development teams and are typically kept up to date so that they are compatible with the different Quarkus versions. However, there are no explicit guarantees, so ensuring the project is active and up-to-date before using them is essential.&lt;/p&gt; &lt;h2&gt;How to add extensions to your Quarkus project&lt;/h2&gt; &lt;p&gt;You can add a Quarkus extension in a few ways:&lt;/p&gt; &lt;ul&gt;&lt;li aria-level="1"&gt;If you're generating a new project using &lt;a href="https://code.quarkus.io/"&gt;code.quarkus.io&lt;/a&gt; or the supported Red Hat build of Quarkus, &lt;a href="https://code.quarkus.redhat.com/"&gt;code.quarkus.redhat.com&lt;/a&gt;, you can select the extensions from the list and they will automatically be added to the generated code starter.&lt;/li&gt; &lt;li aria-level="1"&gt;Using the &lt;a href="https://quarkus.io/guides/cli-tooling"&gt;Quarkus CLI&lt;/a&gt;, you can add extensions using the quarkus extension add command. For example, to add the OpenShift client extension to your project, you would type &lt;code&gt;quarkus extension add quarkus-openshift-client&lt;/code&gt;. (Quarkus will automatically resolve it to &lt;code&gt;quarkus.io:quarkus-openshift-client&lt;/code&gt;).&lt;/li&gt; &lt;li aria-level="1"&gt;If you're developing with VS Code or IntelliJ IDEA, you can also use the Quarkus plug-in to add extensions to your project, as illustrated in Figure 6.&lt;/li&gt; &lt;/ul&gt;&lt;div class="rhd-c-figure"&gt; &lt;article class="media media--type-image media--view-mode-article-content"&gt;&lt;div class="field field--name-image field--type-image field--label-hidden field__items"&gt; &lt;a href="https://developers.redhat.com/sites/default/files/screenshot_from_2023-08-10_14-40-03.png" data-featherlight="image"&gt;&lt;img loading="lazy" src="https://developers.redhat.com/sites/default/files/styles/article_floated/public/screenshot_from_2023-08-10_14-40-03.png?itok=4FhkUJVT" width="600" height="201" alt="How to add extensions with Quarkus Plugin for VS Code" typeof="Image" /&gt;&lt;/a&gt; &lt;/div&gt;&lt;div class="field field--name-field-caption field--type-string field--label-hidden field__items"&gt; &lt;div class="rhd-c-caption field__item"&gt;Figure 6: Select "Quarkus: Add extensions to current project" in the VS Code command palette.&lt;/div&gt; &lt;/div&gt; &lt;/article&gt;&lt;/div&gt; &lt;h2&gt;Go forth and extend your Quarkus application!&lt;/h2&gt; &lt;p&gt;This was a brief introduction to Quarkus extensions. If you want to learn more about Quarkus, check out the &lt;a href="https://developers.redhat.com/products/quarkus/overview"&gt;Quarkus overview on Red Hat Developer&lt;/a&gt;, or get started with Quarkus tutorials on &lt;a href="https://developers.redhat.com/learn"&gt;developers.redhat.com/learn&lt;/a&gt;. If you want to learn even more about how Quarkus extensions work, or perhaps how to build your own extension, you can find more information on &lt;a href="https://quarkus.io/guides/writing-extensions"&gt;quarkus.io&lt;/a&gt;.&lt;/p&gt; The post &lt;a href="https://developers.redhat.com/articles/2023/09/06/introduction-quarkus-extensions-java-dependencies" title="Quarkus extensions give Java dependencies superpowers"&gt;Quarkus extensions give Java dependencies superpowers&lt;/a&gt; appeared first on &lt;a href="https://developers.redhat.com/blog" title="Red Hat Developer"&gt;Red Hat Developer&lt;/a&gt;. &lt;br /&gt;&lt;br /&gt;</summary><dc:creator>Kevin Dubois</dc:creator><dc:date>2023-09-06T07:00:00Z</dc:date></entry><entry><title>Quarkus 3.3.2 released - Maintenance release</title><link rel="alternate" href="&#xA;                https://quarkus.io/blog/quarkus-3-3-2-released/&#xA;            " /><author><name>Guillaume Smet (https://twitter.com/gsmet_)</name></author><id>https://quarkus.io/blog/quarkus-3-3-2-released/</id><updated>2023-09-06T00:00:00Z</updated><published>2023-09-06T00:00:00Z</published><summary type="html">Today, we released Quarkus 3.3.2, our second maintenance release for our 3.3 release train. It includes a bunch of bugfixes, together with documentation improvements. The startup performance/memory regression introduced in 3.3 mentioned in the 3.3.1 announcement should be fixed in this version. Update To update to Quarkus 3.3.2, we recommend...</summary><dc:creator>Guillaume Smet (https://twitter.com/gsmet_)</dc:creator><dc:date>2023-09-06T00:00:00Z</dc:date></entry></feed>
